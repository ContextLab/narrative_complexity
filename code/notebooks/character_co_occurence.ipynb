{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "#import nltk\n",
    "import timecorr as tc\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will be the file that feature extractor notebook generates for each script\n",
    "#out_fn = '/Users/vassiki/Desktop/narrative_complexity/code/notebooks/annotations_with_char_embeddings.csv'\n",
    "out_fn = '/Users/vassiki/Desktop/narrative_complexity/code/notebooks/annotations.csv'\n",
    "out_df = pd.read_csv(out_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chars_from_df(out_df):   \n",
    "    \"\"\"\n",
    "    Function to clean up unique list of characters, will be redundant after\n",
    "    the feature extraction notebook is updated\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    out_df: output csv for each script with segmented events as rows\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    characters: 2d list of unique characters in each event\n",
    "    \"\"\"\n",
    "    characters = []\n",
    "    for row in range(out_df.shape[0]):\n",
    "    \n",
    "        character_per_event = eval(out_df.loc[row, 'characters'])\n",
    "        unique_chars_events = character_per_event.keys()\n",
    "        chars_ev = [' '.join(c.split()) for c in list(unique_chars_events)]\n",
    "        first_name = [c.split(' ')[0] for c in chars_ev]\n",
    "        unique_first = list(dict.fromkeys(first_name))\n",
    "        # pos tag\n",
    "        #pos_tag_names = nltk.pos_tag(unique_first)\n",
    "        #noun_names = [n[0] for n in pos_tag_names if 'VB' not in n[1]]\n",
    "\n",
    "        characters.append(unique_first)\n",
    "    return characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cooccurence_matrix_one_event(characters, event_num=0):\n",
    "    \"\"\"\n",
    "    Function to create cooccurence matrix for all characters in an event.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    characters: 2d list of unique characters in each event\n",
    "    event_num: default 0, row number of event to return cooccurence matrix for\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    co_occurence_df: unique char by unique char dataframe with tallies\n",
    "                        for character cooccurences\n",
    "    \"\"\"\n",
    "    chars = list(dict.fromkeys(sum(characters,[])))\n",
    "    total_array_size = len(chars)\n",
    "    co_occurence_array = np.zeros((total_array_size, total_array_size))\n",
    "    co_occurence_df = pd.DataFrame(co_occurence_array, columns = chars, index=chars)\n",
    "    #all_pairs = list(itertools.combinations(characters[event_num], 2))\n",
    "    all_pairs = list(itertools.permutations(characters[event_num], 2))\n",
    "    for pair in all_pairs:\n",
    "        co_occurence_df.loc[pair[0], pair[1]] += 1\n",
    "    return co_occurence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cooccurence_matrix_all_events(characters):\n",
    "    \"\"\"\n",
    "    Function to create cooccurence matrix across all events\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    characters: 2d list of unique characters in each event\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    co_occurence_df: character by character cooccurence matrix across\n",
    "                        all events\n",
    "    \"\"\"\n",
    "    chars = list(dict.fromkeys(sum(characters,[])))\n",
    "    total_array_size = len(chars)\n",
    "    co_occurence_array = np.zeros((total_array_size, total_array_size))\n",
    "    co_occurence_df = pd.DataFrame(co_occurence_array, columns = chars, index=chars)\n",
    "    for event_chars in range(len(characters)):\n",
    "        #all_pairs = list(itertools.combinations(characters[event_chars], 2))\n",
    "        all_pairs = list(itertools.permutations(characters[event_chars], 2))\n",
    "        for pair in all_pairs:\n",
    "            co_occurence_df.loc[pair[0], pair[1]] += 1\n",
    "    return co_occurence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_one_event_cooccurence(fn, chars, num=0):\n",
    "    \"\"\"\n",
    "    Function to plot each event's cooccurence matrix\n",
    "    \n",
    "    Parameters:\n",
    "    fn: filename to save the co-occurence plot with\n",
    "    chars: 2d list of unique characters in each event \n",
    "    num: default 0, event number\n",
    "    \"\"\"\n",
    "    cdf  = get_cooccurence_matrix_one_event(chars, num)\n",
    "    l = list(cdf.columns)\n",
    "    plt.figure(figsize=(20,15))\n",
    "    sns.heatmap(cdf, xticklabels=l, yticklabels=l)\n",
    "    plt.savefig(fn, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_events_cooccurence(fn, chars):\n",
    "    \"\"\"\n",
    "    Function to plot cooccurence matrix across all events\n",
    "    \n",
    "    Parameters:\n",
    "    fn: filename to save the co-occurence plot with\n",
    "    chars: 2d list of unique characters in each event\n",
    "    \"\"\"    \n",
    "    cca = get_cooccurence_matrix_all_events(chars)\n",
    "    labels = list(cca.columns)\n",
    "    plt.figure(figsize=(20,15))\n",
    "    sns.heatmap(cca, xticklabels=labels, yticklabels=labels)\n",
    "    plt.savefig(fn, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate plot for all events\n",
    "characters = get_chars_from_df(out_df)\n",
    "root_dir = '/Users/vassiki/Desktop/narrative_complexity/notebooks/figures/'\n",
    "fn = os.path.join(root_dir, 'character_occurence.png')\n",
    "plot_all_events_cooccurence(fn, characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate plot for one event at a time\n",
    "characters = get_chars_from_df(out_df)\n",
    "root_dir = '/Users/vassiki/Desktop/narrative_complexity/notebooks/figures/'\n",
    "for event_num in range(len(characters)):\n",
    "    fn = os.path.join(root_dir, 'character_occurence_event_{0}.png'.format(event_num))\n",
    "    plot_one_event_cooccurence(fn, characters, event_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `convert -delay 10 -loop 0 *event*.png cooccurence.gif` in bash to generate gif from all events from the figures subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy example\n",
    "all_elements = np.arange(10)\n",
    "elements_in_events = []\n",
    "num_events = 20\n",
    "for ev in range(num_events):\n",
    "    elems_this_event = np.random.choice(5, np.random.randint(1,5, size=1), replace=False)\n",
    "    elements_in_events.append(list(elems_this_event))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cdf = get_cooccurence_matrix_all_events(elements_in_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = all_cdf.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_check_symmetric(a, tol=1e-8):\n",
    "    return np.all(np.abs(a-a.T) < tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_symmetric(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired my Mark's talk, implementing Correspondence Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/elena-sharova/correspondence_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = get_chars_from_df(out_df)\n",
    "count_df = get_cooccurence_matrix_all_events(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = count_df.gt(10)\n",
    "at= count_df.loc[m.any(axis=1), m.any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleCrosstab = at.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing chi-squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grandTotal = np.sum(sampleCrosstab)\n",
    "correspondenceMatrix = np.divide(sampleCrosstab,grandTotal)\n",
    "rowTotals = np.sum(correspondenceMatrix, axis=1)\n",
    "columnTotals = np.sum(correspondenceMatrix, axis=0)\n",
    "independenceModel = np.outer(rowTotals, columnTotals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chiSquaredStatistic = grandTotal*np.sum(np.square(correspondenceMatrix-independenceModel)/independenceModel)\n",
    "print(chiSquaredStatistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check - compare to scipy Chi-Squared test\n",
    "from scipy.stats import chi2_contingency\n",
    "statistic, prob, dof, ex = chi2_contingency(sampleCrosstab)\n",
    "print(statistic)\n",
    "print(np.round(prob, decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correspondence_analysis(sampleCrosstab):\n",
    "    grandTotal = np.sum(sampleCrosstab)\n",
    "    correspondenceMatrix = np.divide(sampleCrosstab,grandTotal)\n",
    "    rowTotals = np.sum(correspondenceMatrix, axis=1)\n",
    "    columnTotals = np.sum(correspondenceMatrix, axis=0)\n",
    "    independenceModel = np.outer(rowTotals, columnTotals)\n",
    "    chiSquaredStatistic = grandTotal*np.sum(np.square(correspondenceMatrix-independenceModel)/independenceModel)\n",
    "    print('Chi Squared for this matrix is {0}'.format(chiSquaredStatistic))\n",
    "    # pre-calculate normalised rows\n",
    "    norm_correspondenceMatrix = np.divide(correspondenceMatrix,rowTotals[:, None])\n",
    "\n",
    "    chiSquaredDistances = np.zeros((correspondenceMatrix.shape[0],correspondenceMatrix.shape[0]))\n",
    "\n",
    "    norm_columnTotals = np.sum(norm_correspondenceMatrix, axis=0)\n",
    "    for row in range(correspondenceMatrix.shape[0]):\n",
    "        chiSquaredDistances[row]=np.sqrt(np.sum(np.square(norm_correspondenceMatrix\n",
    "                                                        -norm_correspondenceMatrix[row])/columnTotals, axis=1))\n",
    "\n",
    "\n",
    "    standardizedResiduals = np.divide((correspondenceMatrix-independenceModel),np.sqrt(independenceModel))\n",
    "\n",
    "    u,s,vh = np.linalg.svd(standardizedResiduals, full_matrices=False)\n",
    "\n",
    "    deltaR = np.diag(np.divide(1.0,np.sqrt(rowTotals)))\n",
    "\n",
    "    rowScores=np.dot(np.dot(deltaR,u),np.diag(s))\n",
    "    return rowScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rowScores = correspondence_analysis(sampleCrosstab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFirstTwoComponents = pd.DataFrame(data=[l[0:2] for l in rowScores], columns=['X', 'Y'], index=at.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(data=dfFirstTwoComponents,x='X', y='Y', hue=at.columns)\n",
    "ax.axhline(y=0, color='k')\n",
    "ax.axvline(x=0, color='k')\n",
    "ax.get_legend().set_visible(False)\n",
    "\n",
    "for label in at.columns:\n",
    "    plt.annotate(label, \n",
    "                 (dfFirstTwoComponents.loc[label,:]['X'],\n",
    "                  dfFirstTwoComponents.loc[label,:]['Y']),\n",
    "                 horizontalalignment='center', verticalalignment='center',size=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFirstTwoComponents['group'] = list(dfFirstTwoComponents.index)\n",
    "dfFirstTwoComponents['colors'] = ['#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe', '#008080', '#000000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "fig = plt.figure(figsize = (12,8))\n",
    "#plt.figure(figsize = (12,8))\n",
    "ax = sns.scatterplot(dfFirstTwoComponents['X'], dfFirstTwoComponents['Y'])\n",
    "\n",
    "for line in range(0,dfFirstTwoComponents.shape[0]):\n",
    "     ax.text(dfFirstTwoComponents.X[line], dfFirstTwoComponents.Y[line], dfFirstTwoComponents.group[line], horizontalalignment='center', size='large', color= dfFirstTwoComponents.colors[line])\n",
    "        \n",
    "fig.savefig('character2d.png', bbox_inches=\"tight\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('characters_2d.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character co-occurrence matrix weighted by number of mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chars_from_df(out_df):   \n",
    "    \"\"\"\n",
    "    Function to clean up unique list of characters, will be redundant after\n",
    "    the feature extraction notebook is updated\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    out_df: output csv for each script with segmented events as rows\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    characters: 2d list of unique characters in each event\n",
    "    \"\"\"\n",
    "    characters = []\n",
    "    for row in range(out_df.shape[0]):\n",
    "    \n",
    "        character_per_event = eval(out_df.loc[row, 'characters'])\n",
    "        vals = {key: character_per_event[key] for key in character_per_event}\n",
    "        characters.append(vals)\n",
    "    return characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_by_char_all_events(out_df):\n",
    "    char_event_dict = get_chars_from_df(out_df)\n",
    "    row_length = out_df.shape[0]\n",
    "    c = [list(val.keys()) for val in char_event_dict]\n",
    "    chars = list(dict.fromkeys(sum(c,[])))\n",
    "    col_length = len(chars)\n",
    "    print('We have {0} events and {1} characters'.format(row_length, col_length))\n",
    "    row_names = ['Event {0}'.format(n) for n in np.arange(row_length)]\n",
    "    col_names = chars\n",
    "    df = pd.DataFrame(np.zeros((row_length, col_length)), columns=col_names, index=row_names)\n",
    "    \n",
    "    for event_num in range(row_length):\n",
    "        for char_k, val in char_event_dict[event_num].items():\n",
    "            row_label = 'Event {}'.format(event_num)\n",
    "            df.loc[row_label, char_k] += val\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_by_char_one_event(out_df, event_num=0):\n",
    "    char_event_dict = get_chars_from_df(out_df)\n",
    "    row_length = out_df.shape[0]\n",
    "    c = [list(val.keys()) for val in char_event_dict]\n",
    "    chars = list(dict.fromkeys(sum(c,[])))\n",
    "    col_length = len(chars)\n",
    "    row_names = ['Event {0}'.format(n) for n in np.arange(row_length)]\n",
    "    col_names = chars\n",
    "    edf = pd.DataFrame(np.zeros((row_length, col_length)), columns=col_names, index=row_names)\n",
    "    \n",
    "    for char_k, val in char_event_dict[event_num].items():\n",
    "        row_label = 'Event {}'.format(event_num)\n",
    "        edf.loc[row_label, char_k] += val\n",
    "    return edf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_one_event(fn, df, event_num=0):\n",
    "    edf = event_by_char_one_event(df, event_num)\n",
    "    xl = list(edf.columns)\n",
    "    yl = list(edf.index)\n",
    "    plt.figure(figsize=(20,15))\n",
    "    sns.heatmap(edf, xticklabels=xl, yticklabels=yl, cmap=\"BuGn_r\", cbar=False)\n",
    "    plt.savefig(fn, bbox_inches = \"tight\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_events(fn, df):\n",
    "    df = event_by_char_all_events(df)\n",
    "    xl = list(df.columns)\n",
    "    yl = list(df.index)\n",
    "    #plt.figure(figsize=(20,15))\n",
    "    #sns.heatmap(df, xticklabels=xl, yticklabels=yl, cmap=\"BuGn_r\")\n",
    "    plt.figure()\n",
    "    sns.heatmap(df, cmap=\"BuGn_r\")\n",
    "    plt.savefig(fn, bbox_inches = \"tight\")   \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save all the pngs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = '../../figures/'\n",
    "movie_name = 'ten_things'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event_num in range(out_df.shape[0]):\n",
    "    fn = os.path.join(fig_dir, '{0}_event_{1}.png'.format(movie_name, \"{:02d}\".format(event_num)))\n",
    "    plot_one_event(fn, out_df, event_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 49 events and 112 characters\n"
     ]
    }
   ],
   "source": [
    "fn = os.path.join(fig_dir, '{}_all_events.png'.format(movie_name))\n",
    "plot_all_events(fn, out_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using timecorr for plotting dynamic correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def higher_correlation(df, order='first'):\n",
    "    df = event_by_char_all_events(out_df)\n",
    "    mention_matrix = df.values\n",
    "    dyna_mat = get_dynamic_correlations(out_df)\n",
    "    raw_dyna_mat = get_dynamic_correlations_raw(out_df)\n",
    "    \n",
    "    corrs_1 = dyna_mat\n",
    "    corrs_1_delta = raw_dyna_mat\n",
    "    \n",
    "    \n",
    "    # actual graph stuff\n",
    "    corr_1_vec = tc.mat2vec(corrs_1_delta)\n",
    "    corr_1_r = np.asarray(tc.reduce(corr_1_vec, rfun='eigenvector_centrality'))\n",
    "    width = 3\n",
    "    gaussian = {'name': 'Gaussian', 'weights': tc.gaussian_weights, 'params': {'var': width}}\n",
    "    corrs_2 = tc.vec2mat(tc.timecorr(corr_1_r, weights_function=gaussian['weights'], weights_params=gaussian['params']))\n",
    "    \n",
    "    \n",
    "    corrs_2_delta = get_dynamic_correlations_raw(corr_1_r)\n",
    "    corr_2_vec = tc.mat2vec(corrs_2_delta)\n",
    "    corr_2_r = np.asarray(tc.reduce(corr_2_vec, rfun='eigenvector_centrality'))\n",
    "    gaussian = {'name': 'Gaussian', 'weights': tc.gaussian_weights, 'params': {'var': width}}\n",
    "    \n",
    "    \n",
    "    corrs_3  = tc.vec2mat(tc.timecorr(corr_2_r, weights_function=gaussian['weights'], weights_params=gaussian['params']))   \n",
    "    \n",
    "    if order == 'first':\n",
    "        return corrs_1\n",
    "    elif order == 'second':\n",
    "        return corrs_2\n",
    "    else:\n",
    "        return corrs_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimecorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mweights_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0mgaussian_weights\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x1271907a0\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mweights_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minclude_timepoints\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mexclude_timepoints\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcombine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0mnull_combine\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x1270aca70\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcfun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0misfc\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x1270ac710\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrfun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Computes dynamics correlations in single-subject or multi-subject data.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "data: numpy array, pandas dataframe, or a list of numpy arrays/dataframes\n",
       "    Each numpy array (or dataframe) should have size timepoints by features.\n",
       "    If a list of arrays are passed, there should be one array per subject.\n",
       "\n",
       "weights_function: a function of the form func(T, params) where\n",
       "    T is a non-negative integer specifying the number of timepoints to consider.\n",
       "\n",
       "    The function should return a T by T array containing the timepoint-specific\n",
       "    weights for each consecutive time point from 0 to T (not including T).\n",
       "\n",
       "    Default: gaussian_weights; options: laplace_weights, gaussian_weights,\n",
       "    t_weights, eye_weights, mexican_hat_weights\n",
       "\n",
       "weights_params: used to pass parameters to the weights_params function. This\n",
       "    can be specified in any format (e.g. a scalar, list, object, dictionary,\n",
       "    etc.).\n",
       "\n",
       "    Default: None (use default parameters for the given weights function).\n",
       "    Options: gaussian_params, laplace_params, t_params, eye_params,\n",
       "    mexican_hat_params.\n",
       "\n",
       "include_timepoints: determines which timepoints are used to estimate the correlations\n",
       "    at each timepoint.  This is applied after the weights function to further constrain\n",
       "    which timepoints may be considered in computing the correlations at each timepoint.\n",
       "\n",
       "    Options: 'all' (default; include all timepoints), 'pre' (only include timepoints *before*\n",
       "    the given timepoint), 'post' (only include timepoints *after* the given timepoint).\n",
       "\n",
       "exclude_timepoints: additional option, used to filter out any timepoints less than x units\n",
       "    of the timepoint whose correlations are being estimated.  For example, passing\n",
       "    exclude_timepoints=3 will exclude any timepoints 3 or more samples from the given timepoint.\n",
       "    When exclude timepoints is negative, it works inversely-- e.g. exclude_timepoints=-5 will\n",
       "    exclude any timepoints within 5 or fewer samples of the given timepoint.  Real-valued scalars\n",
       "    are supported but are rounded to the nearest Integer.\n",
       "\n",
       "    Default: None (no filtering).\n",
       "\n",
       "combine: a function applied to either a single matrix of vectorized correlation\n",
       "    matrices, or a list of such matrices.  The function should return either\n",
       "    a numpy array or a list of numpy arrays.\n",
       "\n",
       "    Default: helpers.null_combine (a function that returns its input).  Other\n",
       "    useful functions:\n",
       "\n",
       "    helpers.corrmean_combine: take the element-wise average correlations across matrices\n",
       "    helpers.tstat_combine: return element-wise t-statistics across matrices\n",
       "\n",
       "cfun: function to apply to the data array(s)\n",
       "    This function should be of the form\n",
       "    func(data, weights)\n",
       "\n",
       "    The function should support data as a numpy array or list of numpy\n",
       "    arrays.  When a list of numpy arrays is passed, the function should\n",
       "    apply the \"across subjects\" version of the analysis.  When a single\n",
       "    numpy array is passed, the function should apply the \"within subjects\"\n",
       "    version of the analysis.\n",
       "\n",
       "    weights is a numpy array with per-timepoint weights\n",
       "\n",
       "    The function should return a single numpy array with 1 row and an\n",
       "    arbitrary number of columns (the number of columns may be determined by\n",
       "    the function).\n",
       "\n",
       "    Default: A continuous verison of Inter-Subject Functional Connectivity\n",
       "    (Simony et al. 2017).  If only one data array is passed (rather than a\n",
       "    list), the default cfun returns the moment-by-moment correlations for\n",
       "    that array.  (Reference: http://www.nature.com/articles/ncomms12141)\n",
       "\n",
       "rfun: function to use for dimensionality reduction.\n",
       "\n",
       "    All hypertools and scikit-learn functions are supported: PCA, IncrementalPCA, SparsePCA,\n",
       "    MiniBatchSparsePCA, KernelPCA, FastICA, FactorAnalysis, TruncatedSVD,\n",
       "    DictionaryLearning, MiniBatchDictionaryLearning, TSNE, Isomap,\n",
       "    SpectralEmbedding, LocallyLinearEmbedding, MDS, and UMAP.\n",
       "\n",
       "    Can be passed as a string, but for finer control of the model\n",
       "    parameters, pass as a dictionary, e.g.\n",
       "    reduction={‘model’ : ‘PCA’, ‘params’ : {‘whiten’ : True}}.\n",
       "\n",
       "    See scikit-learn specific model docs for details on parameters supported\n",
       "    for each model.\n",
       "\n",
       "    Another option is to use graph theoretic measures computed for each node.\n",
       "    The following measures are supported (via the brainconn toolbox):\n",
       "    eigenvector_centrality, pagerank_centrality, and strength.  (Each\n",
       "    of these must be specified as a string; dictionaries not supported.)\n",
       "\n",
       "    Default: None (no dimensionality reduction)\n",
       "\n",
       "Returns\n",
       "----------\n",
       "corrmats: moment-by-moment correlations\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda2/envs/trial_python3/lib/python3.7/site-packages/timecorr/timecorr.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tc.timecorr?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dynamic_correlations(out_df):\n",
    "\n",
    "    df = event_by_char_all_events(out_df)\n",
    "\n",
    "    mention_matrix = df.values\n",
    "    #print('Original matrix has dimensions'.format(mention_matrix.shape))\n",
    "\n",
    "    # specify kernel\n",
    "    width = 3\n",
    "    gaussian = {'name': 'Gaussian', 'weights': tc.gaussian_weights, 'params': {'var': width}}\n",
    "    vec_corrs = tc.timecorr(mention_matrix, weights_function=gaussian['weights'], weights_params=gaussian['params'])\n",
    "\n",
    "    #print('vectorized shape : ' + str(np.shape(vec_corrs)))\n",
    "\n",
    "    # use the vec2mat function to convert vectorized correlations to moment-by-moment full correlations\n",
    "    mat_corrs = tc.vec2mat(vec_corrs)\n",
    "\n",
    "    #print('matrix shape : ' + str(np.shape(mat_corrs)))\n",
    "    \n",
    "    return mat_corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dynamic_correlations_raw(mat):\n",
    "\n",
    "    #df = event_by_char_all_events(out_df)\n",
    "\n",
    "    #mention_matrix = df.values\n",
    "    mention_matrix = mat\n",
    "    print('Original matrix has dimensions'.format(mention_matrix.shape))\n",
    "\n",
    "    # specify kernel\n",
    "    width = 3\n",
    "    delta = {'name': '$\\delta$', 'weights': tc.eye_weights, 'params': tc.eye_params}\n",
    "    vec_corrs = tc.timecorr(mention_matrix, weights_function=delta['weights'], weights_params=delta['params'])\n",
    "\n",
    "    #print('vectorized shape : ' + str(np.shape(vec_corrs)))\n",
    "\n",
    "    # use the vec2mat function to convert vectorized correlations to moment-by-moment full correlations\n",
    "    mat_corrs = tc.vec2mat(vec_corrs)\n",
    "\n",
    "    #print('matrix shape : ' + str(np.shape(mat_corrs)))\n",
    "    \n",
    "    return mat_corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = '../../figures/'\n",
    "movie_name = 'ten_things'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dyna_mats(out_df, filepath, movie_name):\n",
    "    dyna_mat = get_dynamic_correlations(out_df.values)\n",
    "    df = event_by_char_all_events(out_df)\n",
    "    char_names = df.columns\n",
    "    for tp in range(dyna_mat.shape[2]):\n",
    "        plt.figure(figsize=(20,15))\n",
    "        sns.set(font_scale = 0.8)\n",
    "        #sns.heatmap(dyna_mat[:, :, tp], xticklabels=char_names, yticklabels=char_names, cmap=\"RdBu_r\", vmin=0, vmax=1)\n",
    "        sns.heatmap(dyna_mat[:, :, tp], cmap=\"RdBu_r\", vmin=0, vmax=1)\n",
    "        fn = os.path.join(filepath,'{0}_dyna_mat_event_{1}'.format(movie_name, \"{:03d}\".format(tp)))\n",
    "        plt.savefig(fn, bbox_inches = \"tight\")   \n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dyna_corr_mats(out_df, filepath, movie_name, corr_order='first'):\n",
    "    #dyna_mat = get_dynamic_correlations(out_df.values)\n",
    "    \n",
    "    \n",
    "    df = event_by_char_all_events(out_df)\n",
    "    corr_mat = higher_correlation(df, order=corr_order)\n",
    "    char_names = df.columns\n",
    "    for tp in range(corr_mat.shape[2]):\n",
    "        plt.figure(figsize=(20,15))\n",
    "        sns.set(font_scale = 0.8)\n",
    "        #sns.heatmap(dyna_mat[:, :, tp], xticklabels=char_names, yticklabels=char_names, cmap=\"RdBu_r\", vmin=0, vmax=1)\n",
    "        sns.heatmap(corr_mat[:, :, tp], cmap=\"RdBu_r\", vmin=0, vmax=1)\n",
    "        corr_str = '{}_order_corr'.format(corr_order)\n",
    "        if not os.path.exists(os.path.join(filepath, corr_str)):\n",
    "            os.mkdir(os.path.join(filepath, corr_str))\n",
    "        fn = os.path.join(filepath, corr_str,'{0}_dyna_mat_event_{1}'.format(movie_name, \"{:03d}\".format(tp)))\n",
    "        plt.savefig(fn, bbox_inches = \"tight\")   \n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on lord_of_the_rings:_fellowship_of_the_ring,_the\n",
      "We have 17 events and 191 characters\n",
      "We have 17 events and 191 characters\n",
      "We have 17 events and 191 characters\n",
      "Original matrix has dimensions\n",
      "Original matrix has dimensions\n"
     ]
    }
   ],
   "source": [
    "fn = title_fn[1]\n",
    "movie_title = fn.split('/')[-1].split('.')[0]\n",
    "print(\"Working on {}\".format(movie_title))\n",
    "movie_fig_path = os.path.join(fig_dir, movie_title)\n",
    "out_df = pd.read_csv(fn)\n",
    "plot_dyna_corr_mats(out_df, movie_fig_path, movie_title, corr_order='third')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 49 events and 112 characters\n",
      "Original matrix has dimensions\n",
      "vectorized shape : (49, 6328)\n",
      "matrix shape : (112, 112, 49)\n",
      "We have 49 events and 112 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vassiki/anaconda2/envs/trial_python3/lib/python3.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "plot_dyna_mats(out_df, fig_dir, movie_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating these figures for all scripts we have analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = '../../figures/'\n",
    "root_dir = '../../data/movie_data/'\n",
    "#fns = glob.glob(os.path.join(root_dir,'*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = ['10 Things I Hate About You', 'Lord of the Rings: Fellowship of the Ring, The', 'Forrest Gump', 'Star Wars: The Phantom Menace']\n",
    "title_fn = []\n",
    "for title in titles:\n",
    "    title_str = '_'.join(title.lower().split(' '))\n",
    "    title_fn.append(os.path.join(root_dir, '{}.csv'.format(title_str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_num_characters(fn, df):\n",
    "    xdata = np.arange(df.shape[0])\n",
    "    ydata = df.num_characters\n",
    "    xlabel = 'Event Segment'\n",
    "    ylabel = 'Number of Characters'\n",
    "    plt.figure()\n",
    "    sns.lineplot(x=np.arange(df.shape[0]), y=df.num_characters)\n",
    "    plt.savefig(fn, bbox_inches = \"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_num_sentences(fn, df):\n",
    "    xdata = np.arange(df.shape[0])\n",
    "    ydata = df.num_sentences\n",
    "    xlabel = 'Event Segment'\n",
    "    ylabel = 'Number of Sentences'\n",
    "    plt.figure()\n",
    "    sns.lineplot(x=np.arange(df.shape[0]), y=df.num_sentences)\n",
    "    plt.savefig(fn, bbox_inches = \"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recurring_chars(df, thresh=0.05):\n",
    "    char_mat = event_by_char_all_events(df)\n",
    "    threshold = thresh*np.max(char_mat.sum())\n",
    "    above_thresh = char_mat[char_mat.columns[char_mat.sum()>threshold]]\n",
    "    #characters_above_thresh = above_thresh.columns\n",
    "    #return list(characters_above_thresh)\n",
    "    return above_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_main_characters(fn, df):\n",
    "    df = recurring_chars(df)\n",
    "    plt.figure()\n",
    "    sns.heatmap(df)\n",
    "    plt.savefig(fn, bbox_inches = \"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plots(fn, fig_dir):\n",
    "    df = pd.read_csv(fn)\n",
    "    movie_title = fn.split('/')[-1].split('.')[0]\n",
    "    print(\"Working on {}\".format(movie_title))\n",
    "    movie_fig_path = os.path.join(fig_dir, movie_title)\n",
    "    if not os.path.exists(movie_fig_path):\n",
    "        os.mkdir(movie_fig_path)\n",
    "    \n",
    "        print(\"Plotting character cooccurence...\")\n",
    "        all_events_cooccurence_fn = os.path.join(movie_fig_path, 'character_cooccurence.png')\n",
    "        plot_all_events(all_events_cooccurence_fn, df)\n",
    "    \n",
    "        print(\"Plotting characters per event...\")\n",
    "        char_event_fn = os.path.join(movie_fig_path, 'num_char_per_event.png')\n",
    "        plot_num_characters(char_event_fn, df)\n",
    "    \n",
    "        print(\"Plotting lines per event...\")\n",
    "        line_event_fn = os.path.join(movie_fig_path, 'num_lines_per_event.png')\n",
    "        plot_num_sentences(line_event_fn, df)\n",
    "    \n",
    "        print(\"Plotting main characters...\")\n",
    "        char_fn = os.path.join(movie_fig_path, 'main_characters.png')\n",
    "        plot_main_characters(char_fn, df)\n",
    "        \n",
    "        # dynamic correlations \n",
    "        print(\"Plotting dynamic correlations...\")\n",
    "        dyna_fig_dir = os.path.join(movie_fig_path, 'dyna_plots')\n",
    "        if not os.path.exists(dyna_fig_dir):\n",
    "            os.mkdir(dyna_fig_dir)    \n",
    "        plot_dyna_mats(df, dyna_fig_dir, movie_title)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 10_things_i_hate_about_you\n",
      "Plotting character cooccurence...\n",
      "We have 57 events and 117 characters\n",
      "Plotting characters per event...\n",
      "Plotting lines per event...\n",
      "Plotting main characters...\n",
      "We have 57 events and 117 characters\n",
      "Plotting dynamic correlations...\n",
      "We have 57 events and 117 characters\n",
      "Original matrix has dimensions\n",
      "vectorized shape : (57, 6903)\n",
      "matrix shape : (117, 117, 57)\n",
      "We have 57 events and 117 characters\n",
      "Working on lord_of_the_rings:_fellowship_of_the_ring,_the\n",
      "Plotting character cooccurence...\n",
      "We have 17 events and 191 characters\n",
      "Plotting characters per event...\n",
      "Plotting lines per event...\n",
      "Plotting main characters...\n",
      "We have 17 events and 191 characters\n",
      "Plotting dynamic correlations...\n",
      "We have 17 events and 191 characters\n",
      "Original matrix has dimensions\n",
      "vectorized shape : (17, 18336)\n",
      "matrix shape : (191, 191, 17)\n",
      "We have 17 events and 191 characters\n",
      "Working on forrest_gump\n",
      "Plotting character cooccurence...\n",
      "We have 63 events and 158 characters\n",
      "Plotting characters per event...\n",
      "Plotting lines per event...\n",
      "Plotting main characters...\n",
      "We have 63 events and 158 characters\n",
      "Plotting dynamic correlations...\n",
      "We have 63 events and 158 characters\n",
      "Original matrix has dimensions\n",
      "vectorized shape : (63, 12561)\n",
      "matrix shape : (158, 158, 63)\n",
      "We have 63 events and 158 characters\n",
      "Working on star_wars:_the_phantom_menace\n",
      "Plotting character cooccurence...\n",
      "We have 72 events and 152 characters\n",
      "Plotting characters per event...\n",
      "Plotting lines per event...\n",
      "Plotting main characters...\n",
      "We have 72 events and 152 characters\n",
      "Plotting dynamic correlations...\n",
      "We have 72 events and 152 characters\n",
      "Original matrix has dimensions\n",
      "vectorized shape : (72, 11628)\n",
      "matrix shape : (152, 152, 72)\n",
      "We have 72 events and 152 characters\n"
     ]
    }
   ],
   "source": [
    "#for fn in fns:\n",
    "for fn in title_fn:\n",
    "    create_plots(fn, fig_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gif_paths = glob.glob('../../figures/*/dyna_plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gifs(gif_path):\n",
    "    name='dynamic_char_corr'\n",
    "\n",
    "    images = []\n",
    "    for file in os.listdir(gif_path):\n",
    "        if file.endswith(\".png\"):\n",
    "            images.append(imageio.imread(os.path.join(gif_path, file)))\n",
    "\n",
    "    \n",
    "    gif_outfile = os.path.join(gif_path, name + '.gif')  \n",
    "    imageio.mimsave(gif_outfile, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create for ../../figures/invention_of_lying,_the/dyna_plots\n",
      "create for ../../figures/bourne_identity,_the/dyna_plots\n",
      "create for ../../figures/insomnia/dyna_plots\n",
      "create for ../../figures/s/dyna_plots\n",
      "create for ../../figures/as_good_as_it_gets/dyna_plots\n",
      "create for ../../figures/color_of_night/dyna_plots\n",
      "create for ../../figures/grudge,_the/dyna_plots\n"
     ]
    }
   ],
   "source": [
    "for gif_path in gif_paths:\n",
    "    print(\"create for {}\".format(gif_path))\n",
    "    create_gifs(gif_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bash one liner for making gifs across dirs\n",
    "`for x in $( ls -d $root_dir); do convert -delay 10 -loop 0 \"$x\"*.png \"$x\"animation.gif; done`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
