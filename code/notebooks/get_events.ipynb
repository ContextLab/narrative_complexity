{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:41:37.671722Z",
     "start_time": "2019-08-11T12:41:33.851019Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import html2text\n",
    "import markdown\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import brainiak.eventseg.event as event\n",
    "import hypertools.tools.format_data as fit_transform\n",
    "from num2words import num2words\n",
    "from bs4 import BeautifulSoup\n",
    "from downloader import download_file_from_google_drive as dl\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:41:37.980243Z",
     "start_time": "2019-08-11T12:41:37.673873Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/paxtonfitzpatrick/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords as sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### topic modeling parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:41:37.993712Z",
     "start_time": "2019-08-11T12:41:37.982986Z"
    }
   },
   "outputs": [],
   "source": [
    "n_topics = 100\n",
    "video_wsize = 10 # sentences \n",
    "\n",
    "# vectorizer parameters\n",
    "vectorizer_params = {\n",
    "    'model' : 'CountVectorizer', \n",
    "    'params' : {\n",
    "        'stop_words' : sw.words('english')\n",
    "    }\n",
    "}\n",
    "\n",
    "# topic model parameters\n",
    "semantic_params = {\n",
    "    'model' : 'LatentDirichletAllocation', \n",
    "    'params' : {\n",
    "        'n_components' : n_topics,\n",
    "        'learning_method' : 'batch',\n",
    "        'random_state' : 0,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:41:38.025888Z",
     "start_time": "2019-08-11T12:41:37.997868Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wipe_formatting(script, rehtml=False):\n",
    "    parser = html2text.HTML2Text()\n",
    "    parser.wrap_links = True\n",
    "    parser.skip_internal_links = True\n",
    "    parser.inline_links = True\n",
    "    parser.ignore_anchors = True\n",
    "    parser.ignore_images = True\n",
    "    parser.ignore_emphasis = True\n",
    "    parser.ignore_links = True\n",
    "    text = parser.handle(script)\n",
    "    text = text.strip(' \\t\\n\\r')\n",
    "    if rehtml:\n",
    "        text = text.replace('\\n', '<br/>')\n",
    "        text = text.replace('\\\\', '')\n",
    "    md = markdown.markdown(text)\n",
    "    soup=BeautifulSoup(md,'html5lib')\n",
    "    soup=soup.get_text()\n",
    "    soup = soup.replace('\\n', ' ')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:41:38.052724Z",
     "start_time": "2019-08-11T12:41:38.027504Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filepath, fileid):\n",
    "    data_dir = os.path.dirname(filepath)\n",
    "    \n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        print('downloading data...')\n",
    "        dl(fileid, filepath)\n",
    "        \n",
    "    print('loading data...')\n",
    "    data = pd.read_csv(filepath)\n",
    "    return data.dropna(subset=['script'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:41:38.070495Z",
     "start_time": "2019-08-11T12:41:38.056060Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanup_text(transcript):\n",
    "    lower_nopunc = re.sub(\"[^\\w\\s]+\", '', transcript.lower())    # remove all punctuation\n",
    "    no_digit = re.sub(r\"(\\d+)\", lambda x: num2words(int(x.group(0))), lower_nopunc)    # convert digits to words\n",
    "    spaced = ' '.join(no_digit.replace(',', ' ').split())    # deal with inconsistent whitespace\n",
    "    return spaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:42:27.799262Z",
     "start_time": "2019-08-11T12:42:27.792359Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_windows(transcript, wsize=video_wsize):\n",
    "    cleaned = cleanup_text(wipe_formatting(transcript))\n",
    "    text_list = cleaned.split()\n",
    "    video_w = []\n",
    "    \n",
    "    for ix, word in enumerate(text_list):\n",
    "        video_w.append(' '.join(text_list[ix:ix+wsize]))\n",
    "        \n",
    "    return video_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:41:38.102189Z",
     "start_time": "2019-08-11T12:41:38.091953Z"
    }
   },
   "outputs": [],
   "source": [
    "def topic_model(transcript, vec_params=vectorizer_params, sem_params=semantic_params, return_windows=False):\n",
    "    windows = get_windows(transcript)\n",
    "    traj = fit_transform(windows, vectorizer=vec_params, semantic=sem_params, corpus=transcript)[0]\n",
    "    \n",
    "    if return_windows:\n",
    "        return traj, windows\n",
    "    else:\n",
    "        return traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:41:38.129366Z",
     "start_time": "2019-08-11T12:41:38.104063Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize_k(trajectory, ks_list):\n",
    "    scores = []\n",
    "    \n",
    "    for k in ks_list:\n",
    "        ev = event.EventSegment(k)\n",
    "        ev.fit(trajectory)\n",
    "        w = np.round(ev.segments_[0]).astype(int)\n",
    "        mask = np.sum(list(map(lambda x: np.outer(x, x), t.T)), 0).astype(bool)\n",
    "        within = corrmat[mask].mean()\n",
    "        across = corrmat[~mask].mean()\n",
    "        scores.append((within, across))\n",
    "        \n",
    "    t = list(map(lambda x: x[0]/(x[1]-min([score[1] for score in scores])), scores))\n",
    "    t /= np.max(t)\n",
    "    ratios = list(map(lambda x: x - k/(5*video_wsize), t))\n",
    "    return ks_list[np.argmax(ratios)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:41:38.147481Z",
     "start_time": "2019-08-11T12:41:38.131503Z"
    }
   },
   "outputs": [],
   "source": [
    "def segment_trajectory(traj, k):\n",
    "    ev = event.EventSegment(k)\n",
    "    ev.fit(trajectory)\n",
    "    w = (np.round(ev.segments_[0])==1).astype(bool)\n",
    "    segs = np.array([traj[wi, :].mean(0) for wi in w.T])\n",
    "    event_times = []\n",
    "    for s in ev.segments_[0].T:\n",
    "        tp = np.where(np.round(s)==1)[0]\n",
    "        event_times.append((tp[0], tp[-1]))\n",
    "        \n",
    "    return segs, event_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:41:40.042830Z",
     "start_time": "2019-08-11T12:41:38.151542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n"
     ]
    }
   ],
   "source": [
    "data_filepath = '../../data/data.csv'\n",
    "data_fileID = '1hCCn31z4HM4IzQi59DP-vvUpYKhlvo2S'\n",
    "# possible numbers of events to try for each transcript\n",
    "ks_list = list(range(2, 51))\n",
    "\n",
    "# download and load data\n",
    "data_df = load_data(data_filepath, fileid=data_fileID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T01:25:06.696669Z",
     "start_time": "2019-08-10T23:25:47.136543Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-30eb8496c887>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fit topic model, transform script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trajectory'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3192\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3193\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3194\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-1ca8b4b43964>\u001b[0m in \u001b[0;36mtopic_model\u001b[0;34m(transcript, vec_params, sem_params, return_windows)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtopic_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectorizer_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msem_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msemantic_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_windows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mwindows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_windows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtraj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvec_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msemantic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msem_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_windows\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Dartmouth/CDL/hypertools/hypertools/tools/format_data.py\u001b[0m in \u001b[0;36mformat_data\u001b[0;34m(x, vectorizer, semantic, corpus, ppca, text_align)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# check data type for each element in list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mdtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m# handle text data:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Dartmouth/CDL/hypertools/hypertools/_shared/helpers.py\u001b[0m in \u001b[0;36mget_type\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m'list_str'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# fit topic model, transform script\n",
    "data_df['trajectory'] = data_df.script.apply(topic_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:42:36.952225Z",
     "start_time": "2019-08-11T12:42:36.794792Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18353, 100)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model(data_df.script[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# determine number of events for transcript\n",
    "data_df['n_events'] = data_df.trajectory.apply(optimize_k, ks_list=ks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# segment based on that number\n",
    "data_df['segments'] = data_df.apply(lambda x: segment_trajectory(x['trajectory'], x['n_events']), axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
